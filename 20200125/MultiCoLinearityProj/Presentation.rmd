---
title: "本当はコワい線形回帰"
author: "Kien Knot @ TokyoR Vol. 83"
date: "2020/1/25"
output:   
  ioslides_presentation:
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
set.seed(1234)                       # お気持ちの種
n <- 1000                            # 人
noise <- rnorm(n, mean=0, sd = 0.01) # 気持ちのゆらぎ

X1 <- rnorm(n, mean = 0, sd = 1)     # R言語を触ることによる高揚感
X2 <- rpois(n, lambda = 2)           # 今までに買ったマンガの冊数
X3 <- rnorm(n, mean = 5, sd = 5)     # 美術館の企画展を見たときの感情指数
X4 <- X1 + noise                     # Pythonを触ることによる高揚感
X5 <-X4 + runif(n)                     # Juliaを触ることによる高揚感
#cor(X1, X4)                          # 伏線
# 生きがいスコア
y <- 1.2 * X1 + 0.6 * X2 + 0.3 * X3 + rnorm(n, 0, 1) # 
UseData <- data.frame(IKIGAI          =  y, #生きがいスコア
                      R_usage         = X1, # Rを使う喜び
                      MANGA_count     = X2, # マンガの冊数
                      Museum_emotion  = X3, # 美術館に通う
                      Python_usage    = X4, # Pythonを使う喜び
                      Julia_usage     = X5) # Juliaを使う喜び 
```

## あけ & よろ

- きぬいと([\@0_u0](https://twitter.com/0_u0))
  - 特徴①: 報告する人間の中で一番**TwitterIDが短い**
  - 特徴②: 人間のデータが好物
  - 特徴③: 最近`Julia`を始める
  
- リサーチ企業のアナリスト
  - 悩み: TB規模のデータで機械学習するためのアーキテクチャ構築

- 趣味
  - 簡単なことを難しく考える

# 今日のテーマは「決める」

## 10年は「あっという間」でも「長い」
- 2010年って何？
  - Rのバージョンは2.10.x〜2.11.xだった……
  - きぬいと: Twitter始めて半年の高校1年生(無価値情報)
  - まだ機械学習技術は人間に囲碁で勝てなかった
  - ジョ○ズも存命していた(Wikipedia情報)

- 10年でいろんなことを「決めて」きた
  - 進路
  - 卒論・修論のテーマ
  - 今日の夕飯

## 10年あれば人は「人生の分かれ道」に立ちがち

- 受験「そっか、じゃあ卒業したら、別々だね……」

- 就活「そっか、東京、いっちゃうんだ……」
  - 5年前そう言って友を見送る(院卒なので)

- 転職「そっか、この会社やめちゃうんだね……」
  - 結局まだ在籍している
  
- 結婚「結婚したのか……俺以外のやつと」
  - 最近友人の結婚ラッシュがヤバい。

- 夕食「オーストラリア産黒毛和牛ロースが699円！！」
  - 矛盾塊

## そして迷いがち
- どの選択が「良い」のかわからない
- 「こっちが良さそう」って誰かに言ってほしい
  - そんな都合のいい存在はない。

- そもそも「良い」って何……？
  - 人生って何……？
  - 生きるとは……？
  
- どんどん思考がRegression(退行)。

## Regression？

## Regression(回帰分析)

- データを意思決定に使う話
  - 回帰係数による解釈がしやすいらしい？
  - 昨今知られる機械学習と比べると「ホワイトボックス」
    - 今日の報告群の**完全な下位互換**

- 前提知識: 回帰分析が分かれば大丈夫
  - 回帰**係数**が分かれば大丈夫
  
- そのうちgithubに資料とデータは上がる
  - **寿司🍣のこと**を考えてもらって大丈夫です

# Hey 回帰分析 　　　　　　　　　　私に「生きる意味」を教えて

## 僕はAndroid派です
- 生きる意味、持ってますか？
  - 家族、仕事、友達、R……
  - 自分の生きがいに一番寄与するものを探したい

- 回帰分析で評価・意思決定しよう。
  - どの変数が効くかな？
    - その変数を生きがいに生きていこう
- 「効いている」の基準: p-value
  - 統計的に有意ならそれでいい
  

## データ
- 心の中の自分1,000人にアンケートとった

```{r echo = TRUE}
UseData <- data.frame(IKIGAI          =  y, #生きがいスコア
                      R_usage         = X1, # Rを使う喜び
                      MANGA_count     = X2, # マンガの冊数
                      Museum_emotion  = X3, # 美術館に通う
                      Python_usage    = X4, # Pythonを使う喜び
                      Julia_usage     = X5) # Juliaを使う喜び 
```
- 他データ省略
- このデータに回帰分析を適用する

## きぬいとの生きがいとは……
-  回帰分析してみましょう
- Rでやるには`lm()`関数。
  - `lm_model1`: `R_usage`、`MANGA_count`、`Museum_emotion`のみをつかう

- でもPythonやJuliaのことが気になって仕方がない
  - ついつい投入してしまう
  - `lm_model2`: `Python_usage`、`Julia_usage`をくわえてみる。
  - このモデルで生きる意味を決めてしまおう

```{r echo = TRUE}
lm_model1 <- lm(IKIGAI ~   R_usage       + 
                           MANGA_count   +
                           Museum_emotion,
                           data = UseData)
lm_model2 <- lm(IKIGAI ~ ., data = UseData) #全部使うときは`.`
```


## `lm_model1`の結果
- `Estimate`と`Pr(>|t|)`を確認する
```{r echo = TRUE}
summary(lm_model1)$coefficients
```

- 生きがいはR、マンガ、美術館。
  - ブレはほぼ標準誤差(`Std.Error`)範囲内。
  - p-valueもほぼ0。統計的に有意な結果。
  
## `lm_model2`の結果
- `Estimate`と`Pr(>|t|)`を確認する
```{r echo = TRUE}
summary(lm_model2)$coefficients
```
## `lm_model2`の結果
- `R_usage`は正、`Python_usage`は大きく負！？
  - <s>Rにやきもち妬かれている</s>
    - <s>Juliaはほぼout of 眼中</s>
  - 標準誤差も大きく、統計的にも有意じゃない……
  - p-valueではどちらも「重要じゃない」
- R「あなたの生きがいはマンガと美術館ね。さよなら」  
  
## かなしい
- これからのわたし
  - うず高く積まれたマンガに囲まれ
  - 週末は美術館に行く

- 充実していなくはない……が……
  - でもなぜか心(とディレクトリ)に穴がぽっかり空いてしまった

# BAD END

## 原因は？
- 原因は**多重共線性**
  - 説明変数の中に、互いに相関の強い項目があった
```{r echo=TRUE}
cor(UseData$R_usage, UseData$Python_usage) # 伏線回収
cor(UseData$R_usage, UseData$Julia_usage)
```
  - きぬいとはRもPythonもJuliaも好きだったらしい……
  - 結果推定値の標準誤差がでかくなり有意性を失う。


## **実務でも陥りがち**
- 本来必要なはずの変数
  - p-valueを基準に選ぶと失敗する(こともある)
  
- KPIに寄与する変数を見落とす可能性
  - 説明変数間の相関見ないで回帰すると意思決定ミスります。
  - いろんな水準で変数の重要性を考えよう。

- 決定木でも同じ
  - 多重共線性は精度に大きく影響しない
  - そのくせ重要度ベースでの解釈に大きく関わる。つらい。

  
## まとめ
- **Rは優しい**
  - "結果"は簡単に出してくれる。
  - でも**結果の妥当性**までは教えてくれない
  - 結果を疑おう。
    - ちょっと考えれば**RもPythonもJuliaも等しく愛せること**に気付けるはず
  
- p-valueを過信しないようにしよう
  - 意味がある変数が有意になるとは限らない
    - 「回帰係数は0じゃないことはわかる」だけ。
    
- 生きがいを探そう

## Enjoy!
- 何かを決めるのは難しい
  - 難しいけど[Wantedly貼っとくわ](https://www.wantedly.com/users/70198846)

- 真面目な話をしてしまった……
  - この先の話は[ブログで書いた](http://socinuit.hatenablog.com/entry/2019/12/21/042023)
  - 正則化回帰の話も[ブログで書いた](http://socinuit.hatenablog.com/entry/2019/12/19/132851)
  - Lasso楽しい[ブログも書いた](http://socinuit.hatenablog.com/entry/2019/12/06/015140)
  - 次があれば面白い話をします
